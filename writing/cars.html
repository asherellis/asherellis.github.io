<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <title>Ethics of Autonomous Vehicles</title>
      <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:300,300i,600">
      <link rel="stylesheet" href="https://asherellis.net/css/style.css">
      <link rel="shortcut icon" type="image/png" href="https://asherellis.net/favicons/ico.png">
      <script>
         (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
         (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
         m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
         })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

         ga('create', 'UA-84410223-1', 'auto');
         ga('send', 'pageview');

      </script>
      <style>
         ul {
         list-style: inside circle;
         padding-left: 0
         }
         ul li {
         margin-bottom: .25rem;
         list-style-type: none
         }
         .bullets {
         margin-bottom: .25rem;
         list-style-type: disc;
         margin-left: 5px
         }
         ul ul,
         ul ol {
         margin-top: .25rem;
         margin-bottom: .5rem
         }
         ol {
         list-style: inside decimal;
         padding-left: 0
         }
         ol li {
         margin-bottom: .25rem
         }
         ol ul,
         ol ol {
         margin-top: .25rem;
         margin-bottom: .5rem
         }
      </style>
      <!-- Begin Jekyll SEO tag v2.7.1 -->
      <meta name="generator" content="Jekyll v3.9.0">
      <meta property="og:title" content="Ethics of Autonomous Vehicles">
      <meta name="author" content="Asher Ellis">
      <meta property="og:locale" content="en_US">
      <meta name="description" content="ASHER ELLIS - I study applied math at Yale. I believe technology can fix our civic problems.">
      <meta property="og:description" content="ASHER ELLIS - I study applied math at Yale. I believe technology can fix our civic problems.">
      <link rel="canonical" href="https://asherellis.net/writing/AVs/">
      <meta property="og:url" content="https://asherellis.net/writing/gapyear/">
      <meta property="og:site_name" content="Asher Ellis">
      <meta name="twitter:card" content="summary">
      <meta property="twitter:title" content="Ethics of Autonomous Vehicles">
      <meta name="twitter:site" content="@asher__ellis">
      <meta name="twitter:creator" content="@Asher Ellis">
      <script type="application/ld+json">
         {"author":{"@type":"Person","name":"Asher Ellis"},"description":"My name is Asher Ellis - this is my homepage.","url":"https://asherellis.net/writing/cars/","@type":"WebPage","headline":"Ethics of Autonomous Vehicles","@context":"https://schema.org"}
      </script>
      <!-- End Jekyll SEO tag -->
   </head>
   <body>
      <div class="container">
         <link href="https://fonts.googleapis.com/css?family=PT+Sans:400,700" rel="stylesheet" type="text/css">
         <header class="masthead">
            <h1 class="masthead-title">
               <strong><a href="https://asherellis.net/">Asher Ellis</a></strong>
            </h1>
            <nav class="masthead-nav">
               <ul class="navlist">
                  <li class="currnavitem"><a href="https://asherellis.net/writing">Writing</a></li>
                  <li class="navitem"><a href="https://asherellis.net/links">Links</a></li>
                  <li class="navitem"><a href="https://asherellis.net/photos">Photos</a></li>
                  <li class="navitem"><a href="https://asherellis.net/work">Work</a></li>
                  <li class="navitem"><a href="https://asherellis.net/about">About</a></li>
               </ul>
            </nav>
         </header>
         <div class="content post">
            <h1 class="page-title">Ethics of Autonomous Vehicles</h1>
            <p>Every year, 1.35 million people die from road crashes worldwide. Self-driving cars, or autonomous vehicles (AVs), will not only reduce this rate drastically, but they will also fundamentally transform how people live and work. They will shorten commute times, enable people to live further from cities, reduce harmful gas emissions, and free up road and parking space for other uses. But implementing this technology presents a myriad of ethical challenges.</p>
            <h4 style="color:#888;">Why Trolley Problems are Morally Relevant</h4>
            <p>There is a view that trolley problems – scenarios where a vehicle must “choose” one of
               several options that will cause harm in a collision – are not relevant for the ethics of self-driving
               cars. This view is typically defended through one of four arguments. Below I will summarize
               each of these arguments, as well as University of Cambridge researcher Geoff Keeling’s
               counterarguments.
            </p>
            <p><em>Not Going to Happen</em> Argument: Self-driving cars will not encounter trolley problem-like cases. Therefore, trolley
               problems are not relevant for the development of the ethics of self-driving cars.
            </p>
            <p>It is possible that AVs will encounter trolley problem-like scenarios, especially if the use
               of the vehicles becomes widespread. For example:
            </p>
            <p style="margin-left:10%; margin-right:10%;">“The AV is travelling on a two-lane bridge. A bus in the other lane swerves into the AV’s
               lane. The AV can either brake, in which case it will collide with the bus; or it can swerve
               into the other lane, in which case it will hit the side of the bridge. Cases like these seem
               plausible” (Keeling).
            </p>
            <p>Even if AVs don't encounter trolley problem-like scenarios, such cases may still be
               relevant. Studying idealized scenarios can provide valuable insight, even if the real world
               presents non-idealized scenarios. For example, physicists and chemists use the ideal gas law
               (PV=nRT) to study the fundamental nature of gases, even though gases exhibit non-ideal
               behaviors in practice. In the context of self-driving cars, studying idealized trolley problems can
               provide insight into topics such as whether there is a moral difference between killing and letting
               die. Such insight can be applicable towards the ethics of self-driving cars, even in cases that
               don’t resemble an idealized trolley problem.
            </p>
            <p><em>Moral Difference</em> Argument: There is a moral difference between trolley cases and the real-world cases a self-driving car would face. In trolley problems, there is no information on who is guilty, and none of the actors have moral obligations. With self-driving cars, one can examine the events leading up to the case and determine who is responsible. Additionally, the manufacturers might have legal obligations for the welfare of the AVs’ passengers. These factors, which are not present in trolley problems, influence the moral permissibility of the AVs’ behaviors.
               Another difference between trolley problems and the situations self-driving cars would face is probability. Trolley problems depend on certainty of outcome. But with self-driving cars, the consequences of each course of action can only be estimated with a probability.
            </p>
            <p>For the guilt-responsibility difference between trolley problems and cases AVs would encounter, this is merely an additional moral property present for AVs that is not present in trolley problems. An extra moral property does not diminish the relevance of the other moral properties shared by both trolley problems and self-driving cars. The moral properties that are shared can be studied, irrespective of the moral properties that are not shared.</p>
            <p>For the certainty of outcome difference between trolley problems and the cases AVs would encounter, this changes the moral calculation process. Yet, that does not render trolley problem cases irrelevant. With a scenario that involves risk and probability, the self-driving car can take both the probability and severity of each outcome into account when “calculating” which course of action to take. This is an added layer of complexity, yet the general moral questions are the same. Thus, one can still analyze trolley problem scenarios to address the moral questions of self-driving cars.</p>
            <p><em>Impossible Deliberation</em> Argument: Self-driving cars cannot solve moral dilemmas based on trolley problem cases because they typically use a bottom-up approach to decision-making, whereas trolley problems are based on a top-down approach.</p>
            <p>When designing self-driving cars, developers can use a “value-sensitive design process.” Engineers, stakeholders, regulators, and moral philosophers would work cooperatively to determine the ethical implications of the design choices for AVs. These technological specifications would be applied to ensure the “final product” reflects our moral values. Discussions such as the trolley problem can be used to help designers better understand the ethics behind the decisions AVs will need to make.</p>
            <p>For example, discussions on the trolley problem led Frances Kamm to develop the Principle of Permissible Harm. This principle describes patterns in our intuitions in trolley cases, which provides insight into what deems an act morally permissible or impermissible. Thus, the trolley problem is relevant for the moral theories that emerge, since those ideas can be applied towards the value-sensitive design process of self-driving cars.</p>
            <p><em>Wrong Question</em> Argument: “The values which ought to be encoded into AV decision-making algorithms are not determined by moral considerations.” Trolley problems may provide moral insight into AV collisions, but that isn’t what we need.
               The true issue is achieving broad societal acceptance for the underlying moral principles. People will not comply with a system where they disagree with the moral values encoded. As long as this barrier exists, the trolley problem is irrelevant.
            </p>
            <p>Most people accept legal moralism, in which laws (e.g. "it is illegal to steal") are based on moral grounds. Because the ethics of self-driving cars are nuanced, there will be people who disagree with their legal-ethical framework. However, it will not be at such a level that broad social acceptance would become an issue.</p>
            <p>Additionally, although broad social acceptance is a factor in the AV design process, it is not <em>primarily</em> a social dilemma. If public opinion holds that AVs should behave according to immoral principles, broad social acceptance is insufficient. But if the public’s view of how AVs should behave properly aligns with good moral principles, AVs’ choices should reflect those views. Both of these conclusions are based on the moral principles, not their social reception. Thus, the social acceptance factor is a distinct issue from the moral considerations of trolley problem cases. Although trolley problems cannot solve every aspect of the development of self-driving cars (such as the social ones), they can provide insight into the moral challenges.</p>
            <h4 style="color:#888;">Deontological Approach</h4>
            <p>In a deontological approach, self-driving cars would follow duty-bound principles, regardless of the consequences, to determine the most moral course of action in trolley problem-like scenarios. To analyze this, I will focus on the scenario depicted in the image below. Assuming the self-driving car cannot brake, should it continue on its natural course and let five people die? Or should it intervene to change its course and kill one person?</p>
            <img class="book" src="https://cdn.technologyreview.com/i/images/scenario1_0.png" alt="" height="320"></img>
            <p>According to the most basic form of deontological ethics, the car would continue on its natural, default course and let five people die. The AV must follow duty-bound principles (perhaps defined by Kant’s categorical imperative) such as “do not kill.”</p>
            <p>In a more sophisticated form of the deontological argument, one could claim that there is an intrinsic difference between killing versus letting die. Killing is a violation of a negative duty to refrain from killing. Letting die is a violation of the positive duty to save lives. According to philosopher Raymond A. Belliotti, here are the possible relationships between positive and negative duties:</p>
            <ol>
               <li>“All negative duties are equally obligatory, and to violate a negative duty is a morally worse act than to violate a positive duty.”</li>
               <li>“Any violation of a negative duty is a morally worse act than any violation of a positive duty.”</li>
               <li>“Any violation of a particular negative duty is a morally worse act than any violation of its correlated positive duty.”</li>
               <li>“Some violations of a particular negative duty are morally worse acts, ceteris paribus, than some violations of its correlated positive duty.”</li>
               <li>“Any violation of a particular negative duty is a morally worse act, ceteris paribus, than any violation of its correlated positive duty” (Belliotti).</li>
            </ol>
            <p>If violating the negative duty to not kill is an intrinsically worse act than the violating positive duty to save lives, then the self-driving car should let five people die rather than kill one person.</p>
            <p>As one takes the deontology approach to the limits, it becomes more difficult to fully embrace deontology. One might ask, can the self-driving car switch its course to save 10 lives? 100? 1,000? Can a self-driving car break the law (e.g. cross a double yellow line, run a red light, etc.) to save lives or mitigate the risk of killing? A strict deontologist would answer “no” to those questions, unless they had other Kantian duties that outweighed the duty to not kill.</p>
            <p>Subjectivity is another factor that makes it difficult to fully embrace deontological ethics.
               Who gets to decide what duty-bound principles are “correct”? Even if people agree upon the letter of the moral principles, there may be disagreement on what those principles entail. In this example, there may be disagreement on whether “do not kill” encompasses preventing death from occurring.
            </p>
            <h4 style="color:#888;">Utilitarian Approach</h4>
            <p>In a utilitarian approach to trolley problem cases, self-driving cars would determine the most moral course of action based on what allows “the greatest amount of good for the greatest number of people.” To analyze this, I will focus again on the simple scenario presented above. Should the self-driving car continue on its natural course and let five people die, or should it intervene to change its course and kill one person?</p>
            <p>According to the most basic form of utilitarian ethics – Bentham’s act utilitarianism – the car should choose the action that minimizes total harm and produces the best possible outcome. It should change its course and kill one person to save the other five. Here, unlike in the deontological approach, action is morally equivalent to inaction, as they have the same consequence.</p>
            <p>Another form of utilitarianism is rule utilitarianism, where morality is based on choosing the action that aligns with the rule that produces the best possible outcome. This theory overlaps with deontology, both being rule-based. However, deontological duties stem from something intrinsic and absolute, while rule utilitarianism is based on rules that have utility.</p>
            <p>In “The Moral Landscape,” neuroscientist and philosopher Sam Harris offers a more sophisticated form of utilitarian ethics. Harris argues that morality is based on consciousness. In a “space of peaks and valleys, where the peaks correspond to the heights of flourishing possible for any conscious system, and the valleys correspond to the deepest depths of misery,” the goal of morality is to maximize the peaks and minimize the valleys (Harris). The answers to ethical questions should be whatever realizes that goal. If humans can measure and quantify people’s conscious states – which is becoming possible through technologies such as fMRI – theoretically, we can apply science (genetics, neuroscience, economics, sociology, etc.) to answer ethical questions.</p>
            <p>Harris’s theory relates to the utilitarian approach to solving AV trolley problem scenarios. It calls for a broader account of the consequences of each course of action, beyond just the number of deaths. It accounts for every experience by conscious beings that was affected by the course of action. This includes the emotional trauma inflicted on the bystanders, the car passenger, and the passenger’s family members. It includes the legal issues caused by the collision. It includes the ramifications of making exceptions to principles such as natural rights and equal protection (which inevitably occurs when it is permitted to kill one innocent person to save five). It includes the differences in how the death of one person would impact aggregate wellbeing versus the death of another person. For example, the death of an important political or business figure might be worse than the death of a homeless person. The death of a person with many loved ones might be worse than the death of a person with few loved ones. Yet, the act of assigning people moral values, which determine the importance of preserving them in trolley cases, might bring negative ramifications (e.g. privacy issues, emotional discomfort) that outweigh the first-order benefits.</p>
            <h4 style="color:#888;">Random Approach</h4>
            <p>In a random approach, self-driving cars would determine the most moral course of action based on random selection. This approach, initially proposed by Kyoto University informatics researchers Liang Zhao and Wenlong Li, is far less mainstream than the deontological and utilitarian approaches.</p>
            <p>I will focus again on the simple scenario presented in the image above. In the random selecting approach, the car would randomly select between continuing on its natural course, thereby letting five people die, and intervening to change its course and kill one person.</p>
            <p>Only after considering the shortcomings of the other two approaches does random selecting appear reasonable. In the deontological approach, it is naive to allow harm to occur to five people based on the negative duty to not kill. By “doing nothing” (allowing the car to continue on its default path), a choice is still being made. The car is still killing people, even if it wasn’t changing its path. Another general shortcoming of the deontological approach, as Zhao and Li argue, is that there is no duty-bound principle that works for all situations. There will always be exceptions to the rules, as well as disagreement on what the rules should be.</p>
            <p>The utilitarian approach also has its drawbacks. First, one cannot ever establish scientific certainty about which course of action maximizes utility. The calculation of each course of action’s utility is too complex, involving many qualitative factors. Second, the utilitarian approach’s ranking system (prioritizing what types of people should be saved in collisions) poses other ethical challenges. One one hand, young and healthy people should be preserved over the old and sick. People with important business or political roles should be prioritized, since other people depend on them. But as Zhao and Li describe, “this approach leads to challenges to other more fundamental principles such as ‘Everyone should be equal’” (Zhao and Li 3). The families and friends of collision victims would know that their loved one was killed by a computer algorithm deliberately designed by someone else. People would feel discriminated against, subjected to a system that systematically favors or disfavors them.</p>
            <p>The most significant benefit for the random approach is that it is practical. After the first few years of implementation, society can reevaluate whether the utilitarian or deontological approach is better based on experience. During that time, we would also learn how rare trolley
               cases are, which would impact how comfortable we are accepting random selecting as a permanent solution.
            </p>
            <p>Another benefit of the random approach is that nobody is to blame for collisions. People wouldn’t be subjected to an algorithm that favors or disfavors them. Zhao and Li argue that “random choice is similar to the case of negligent homicide, whose definition is the killing of another person through gross negligence or without malice… It is quite different to kill someone because of bias, which may equal to murder” (Zhao and Li 4).</p>
            <p>I found one issue with random selecting that Zhao and Li failed to address. Sartre, a key figure in existentialist philosophy, argued, “if I do not choose, I am still choosing” (Sartre 54).
               One cannot opt out of moral choice, as that is a choice in itself. The random selecting approach is more disconnected from direct choice than the deontological approach, but it still runs into the same inescapable problem of choice.
            </p>
            <h4 style="color:#888;">Who gets to Decide the Ethical Framework?</h4>
            <p>Unlike normal cars, self-driving cars enable the car designers and manufacturers to determine the ethics of the vehicles. This raises the questions – how much control should the technology corporations have, and who should have a voice in the moral design process?</p>
            <p>Inevitably, the technologists and designers will have significant control. But ideally, based on Rousseau’s idea of democracy – that law should be based on the general will of the society – the public should also have some input.</p>
            <p>Another question that would influence the development of an ethical framework of AVs is of the nature of moral truth. One view is ethical absolutism – that there are universally valid and invalid truths about what is right and wrong. According to this view, there should be a single set of ethical rules for all self-driving cars. This makes it difficult to rely on surveys, because the responses to ethical questions would vary between different societies. Thus, an absolutist might rely more heavily on the input of experts (technologists, philosophers, scientists) than the public’s general will.</p>
            <p>According to ethical relativism, moral truths are only true relative to a society. The ethical norms of one society could be different than those in another society, yet both could be valid.
               This theory would permit different companies and countries to decide what ethics are encoded in their self-driving cars. An ethical relativist could value public opinion more heavily, since they accept that ethical norms can vary by society while still being valid.
            </p>
            <p>Although absolutism can be connected to relying on experts and relativism can be connected to relying on the public, the connection isn’t perfect. In other words, even if someone confirmed whether ethical absolutism or relativism is true, that doesn’t necessarily mean we should rely solely on experts or the public, respectively.</p>
            <p>For ethical absolutism, different experts may have contrasting views on the same ethical issue. Even if agreement is met between all people, that agreement might be different from the ethical views that succeed it in the future. Also, people’s views of the truth will only ever be epistemology. We can never confirm with certainty whether our epistemological understanding aligns with the “absolute” ontology. These nuances make it difficult to develop an ethical framework under absolutism.</p>
            <p>For ethical relativism, the public may hold views that are truly harmful in practice. For example, people might believe that slavery is a positive institution, or that we should hunt for witches. Relativism does not allow one to rebut these ideas. Additionally,
               relativism can be self-contradicting. If no view can be definitively rejected, a relativist cannot rule out the possibility that absolutism is correct.
            </p>
            <p>One theory that combines absolutism and relativism is ethical pluralism. Pluralism holds that there are absolute truths, yet those truths can be understood and actualized in different ways. There can also be multiple truths that conflict with one another but are simultaneously correct. According to this view, there is not a single valid approach for designing the ethical framework of self-driving cars, but there can be invalid ones.</p>
            <h4 style="color:#888;">MIT Moral Machine</h4>
            <p>MIT’s Media Lab conducted a survey experiment called “Moral Machine” to provide insight into people’s ethical priorities for self-driving cars. Using a game-like interface, the experiment showed people trolley problem-like scenarios and asked them how the self-driving car would respond.</p>
            <p>In the results, there was general agreement on just these two principles: prioritize saving more lives over less lives, and prioritize saving humans over animals. For everything else, there was strong disagreement between different countries. Here are some of the points of disagreement:</p>
            <ul>
               <li> –  Prioritize saving passengers or pedestrians?</li>
               <li> –  Prioritize saving younger people or older people?</li>
               <li> –  Prioritize saving women or men?</li>
               <li> –  Prioritize saving healthy or sick people?</li>
               <li> –  Prioritize saving higher social status or lower?</li>
               <li> –  Prioritize saving law-breakers or law-abiders?</li>
               <li> –  Should the car change its course (take action) or stay on course (inaction)?</li>
            </ul>
            <p>The results of the Moral Machine experiment highlight how complex the moral designprocess is. Obviously, there should also be reasonable boundaries, determined by both the general will of the public (as determined by surveys) and experts (moral philosophers, stakeholders, technologists, etc.). Yet because the preferences in the survey were so mixed, I believe individuals should be able to customize certain settings. The ability to make decisions within ethical certain constraints is not much of a change from the present driving situation.People are already required to operate within the constraints of the law while maintaining responsibility over their own ethical choices. My solution would preserve this precedent.</p>
            <h4 style="color:#888;">Tragedy of the Commons</h4>
            <p>In a trolley problem scenario, should a self-driving car be able to sacrifice a passenger’s life to save a greater number of people (based on utilitarian ethics)?</p>
            <p>In a <em>Science</em> study, participants were asked this question. They answered that self-driving cars should prioritize saving the most amount of lives. However, when asked whether they would purchase this type of vehicle, participants said they wouldn’t.</p>
            <p>This makes it difficult to implement strict utilitarianism in the ethical framework of self- driving cars. By prioritizing saving the greatest number of lives, it would likely take longer for society to agree on the norms for self-driving cars. During this delay, people would still be using normal cars, so people would continue dying at a higher rate from car accidents. Thus, by trying to save more lives through utilitarianism, less lives might be saved.</p>
            <p>This scenario is known as a “tragedy of the commons.” By acting independently in the pursuit of their own interests, the collective good is spoiled. Philosophically, this study serves as a counterexample to egoism and rational egoism. For the implementation of self-driving cars, this study reveals one of the social issues that might emerge.</p>
            <p>Few solutions have been proposed to circumvent this tragedy of the commons. In a podcast episode, Sam Harris and Paul Bloom proposed a lack of transparency as a solution. If potential customers weren’t aware that their cars could sacrifice them, they might be more likely to purchase the cars.</p>
            <p>I don't think this is the optimal solution, as people will demand to know this information. Instead of hiding the morals, I'd argue there should be a social contract. As described by Hobbes, Locke, and Rousseau, the social contract is a model in which individuals agree to standards (i.e. moral, political, cultural norms) that enable a society to function. In one version of this social contract, people might agree to purchase cars that followed utilitarian ethics. Alternatively, the contract could allow individuals to <em>choose</em> whether their cars should prioritize saving the passenger or the greatest number of lives. In this version, passengers would be legally liable if they were the passenger in a collision – similar to the legal situation for non-self-driving cars.</p>
            <h4 style="color:#888;">Works Cited</h4>
            See <a href="https://asherellis.net/writing/AVs_citations.pdf">here</a>.
         </div>
      </div>
   </body>
</html>
